{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install ollama\n",
    "\n",
    "import pandas as pd\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='codestral:22b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model response\n",
    "\n",
    "\"What a great question!\n",
    "\n",
    "The sky appears blue to our eyes because of the way that light behaves when it interacts with the Earth's atmosphere. Here's a simplified explanation:\n",
    "\n",
    "1. **Sunlight**: The sun emits white light, which is made up of all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
    "2. **Atmosphere**: When sunlight enters the Earth's atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules scatter the light in all directions.\n",
    "3. **Scattering**: The shorter, blue wavelengths of light are scattered more than the longer, red wavelengths by these gas molecules. This is known as Rayleigh scattering, named after the British physicist Lord Rayleigh, who first described the phenomenon in the late 19th century.\n",
    "4. **Blue dominance**: As a result of this scattering, the blue light is dispersed throughout the atmosphere, giving the sky its blue appearance. The more blue light that's scattered, the bluer the sky will appear.\n",
    "5. **Red light prevails**: Meanwhile, the longer wavelengths of red and orange light continue to travel in a more direct path to our eyes, with less scattering. This is why the sun itself appears yellow or orange, as our atmosphere scatters away some of the shorter blue wavelengths.\n",
    "6. **Atmospheric conditions**: The color of the sky can be influenced by various atmospheric conditions, such as:\n",
    "\t* Dust and pollution particles: These can scatter light in different ways, making the sky appear more hazy or gray.\n",
    "\t* Water vapor: High humidity can lead to a more diffuse scattering of light, resulting in a whiter or grayer sky.\n",
    "\t* Clouds: Thick clouds can reflect and scatter sunlight, changing the apparent color of the sky.\n",
    "\n",
    "In summary, the sky appears blue because of the selective scattering of shorter blue wavelengths by the tiny gas molecules in our atmosphere. The exact shade of blue can vary depending on atmospheric conditions, but the fundamental principle remains the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model='codellama:13b',\n",
    "    messages=[{'role': 'user', 'content': '''create a xgboost 2 estimation for a\n",
    "               timeseries. use modern framworks. \n",
    "               make the code short, standalone and consise. \n",
    "               only return the code, no explaination. add one example'''}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "data = np.random.randn(100, 3)\n",
    "df = pd.DataFrame(data, index=dates, columns=['feature1', 'feature2', 'target'])\n",
    "\n",
    "# Preprocess data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Split data into features and target\n",
    "features = df_scaled[:, :-1]\n",
    "target = df_scaled[:, -1]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(0.7 * len(df))\n",
    "train_features, test_features = features[:train_size], features[train_size:]\n",
    "train_target, test_target = target[:train_size], target[train_size:]\n",
    "\n",
    "# Create XGBoost regressor\n",
    "xgb_reg = XGBRegressor()\n",
    "\n",
    "# Train model on training data\n",
    "xgb_reg.fit(train_features, train_target)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = xgb_reg.predict(test_features)\n",
    "\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(test_target, predictions)\n",
    "r2 = r2_score(test_target, predictions)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(df.index[train_size:], test_target, label='Actual')\n",
    "plt.plot(df.index[train_size:], predictions, label='Predicted')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Scaled Target')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function\n",
    "def gpt(prompt=\"return hi, nothing else\"):\n",
    "  response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': prompt,\n",
    "  },\n",
    "  ])\n",
    "  return response['message']['content']\n",
    "\n",
    "\n",
    "print(gpt(\"How can I be like Steve Jobs?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "messages = []\n",
    "\n",
    "def send(chat):\n",
    "  messages.append(\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': chat,\n",
    "    }\n",
    "  )\n",
    "  stream = ollama.chat(model='llama3', \n",
    "    messages=messages,\n",
    "    stream=True,\n",
    "  )\n",
    "\n",
    "  response = \"\"\n",
    "  for chunk in stream:\n",
    "    part = chunk['message']['content']\n",
    "    print(part, end='', flush=True)\n",
    "    response = response + part\n",
    "\n",
    "  messages.append(\n",
    "    {\n",
    "      'role': 'assistant',\n",
    "      'content': response,\n",
    "    }\n",
    "  )\n",
    "\n",
    "  print(\"\")\n",
    "\n",
    "while True:\n",
    "    chat = input(\">>> \")\n",
    "\n",
    "    if chat == \"/exit\":\n",
    "        break\n",
    "    elif len(chat) > 0:\n",
    "        send(chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"llama3\",\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "  }\n",
    "]\n",
    "\n",
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
    "\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
    "# Load LLM inference endpoints from an env variable or a file\n",
    "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
    "# and OAI_CONFIG_LIST_sample\n",
    "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
    "# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\n",
    "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
    "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
    "# This initiates an automated chat between the two agents to solve the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "# Configuration list for llama3 model\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"llama3\",\n",
    "        \"base_url\": \"http://localhost:11434/v1\",\n",
    "        \"api_key\": \"ollama\",\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize the intern (Assistant Agent)\n",
    "intern = AssistantAgent(\"intern\", llm_config={\"config_list\": config_list})\n",
    "\n",
    "# Initialize the manager (Warren Buffett)\n",
    "manager = AssistantAgent(\"WarrenBuffett\", llm_config={\"config_list\": config_list})\n",
    "\n",
    "# Initialize the user proxy agent\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False})\n",
    "\n",
    "# Define the messages for generating an equity research paper\n",
    "intern_message = \"\"\"\n",
    "Please write a draft of an equity research paper that includes the following:\n",
    "1. Background on the operations of the business.\n",
    "2. A table with the stock price information for the company, including:\n",
    "   - Date\n",
    "   - Opening Price\n",
    "   - Closing Price\n",
    "   - High\n",
    "   - Low\n",
    "   - Volume\n",
    "3. Provide a detailed analysis of the stock performance and future outlook.\n",
    "\n",
    "Company: Apple Inc\n",
    "\"\"\"\n",
    "\n",
    "manager_message = \"\"\"\n",
    "Provide feedback on the draft equity research paper written by the intern. Make sure to use a fundamental research approach and give detailed input on areas that need improvement or further analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Simulate the intern doing the work\n",
    "intern_response = user_proxy.initiate_chat(intern, message=intern_message)\n",
    "\n",
    "# Simulate the manager providing feedback\n",
    "manager_response = user_proxy.initiate_chat(manager, message=manager_message)\n",
    "\n",
    "# Print out the intern's draft and the manager's feedback\n",
    "print(\"Intern's Draft:\\n\", intern_response)\n",
    "print(\"\\nManager's Feedback:\\n\", manager_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_market_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mget(api_endpoint, params\u001b[38;5;241m=\u001b[39mparams)\u001b[38;5;241m.\u001b[39mjson()  \u001b[38;5;66;03m# Return the API response as JSON\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Now you can call this function with your desired ISIN code and retrieve market information!\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m market_info \u001b[38;5;241m=\u001b[39m \u001b[43mget_market_info\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSE0000145005\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your desired ISIN code\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(market_info)  \u001b[38;5;66;03m# Print the returned market information\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_market_info' is not defined"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
